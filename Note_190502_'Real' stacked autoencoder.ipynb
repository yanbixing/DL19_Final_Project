{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real stacked autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hook method is totally son of bitch...\n",
    "\n",
    "Easy and swift... But two naive and crude...\n",
    "\n",
    "WTF!!!!!!!!!!!!!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T23:58:07.974500Z",
     "start_time": "2019-05-03T23:58:07.971336Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T23:56:47.643829Z",
     "start_time": "2019-05-03T23:56:47.621814Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CDAutoEncoder(nn.Module):\n",
    "    r\"\"\"\n",
    "    Convolutional denoising autoencoder layer for stacked autoencoders.\n",
    "    This module is automatically trained when in model.training is True.\n",
    "    Args:\n",
    "        input_size: The number of features in the input\n",
    "        output_size: The number of features to output\n",
    "        stride: Stride of the convolutional layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, stride):\n",
    "        super(CDAutoEncoder, self).__init__()\n",
    "\n",
    "        self.forward_pass = nn.Sequential(\n",
    "            nn.Conv2d(input_size, output_size, kernel_size=2, stride=stride, padding=0),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.backward_pass = nn.Sequential(\n",
    "            nn.ConvTranspose2d(output_size, input_size, kernel_size=2, stride=2, padding=0), \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Train each autoencoder individually\n",
    "        x = x.detach()\n",
    "        # Add noise, but use the original lossless input as the target.\n",
    "        x_noisy = x * (Variable(x.data.new(x.size()).normal_(0, 0.1)) > -.1).type_as(x)\n",
    "        y = self.forward_pass(x_noisy)\n",
    "\n",
    "        if self.training:\n",
    "            x_reconstruct = self.backward_pass(y)\n",
    "            loss = self.criterion(x_reconstruct, Variable(x.data, requires_grad=False))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        return y.detach()\n",
    "\n",
    "    def reconstruct(self, x):\n",
    "        return self.backward_pass(x)\n",
    "\n",
    "\n",
    "class StackedAutoEncoder(nn.Module):\n",
    "    r\"\"\"\n",
    "    A stacked autoencoder made from the convolutional denoising autoencoders above.\n",
    "    Each autoencoder is trained independently and at the same time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(StackedAutoEncoder, self).__init__()\n",
    "\n",
    "        self.ae1 = CDAutoEncoder(3, 128, 2)\n",
    "        self.ae2 = CDAutoEncoder(128, 256, 2)\n",
    "        self.ae3 = CDAutoEncoder(256, 512, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = self.ae1(x)\n",
    "        a2 = self.ae2(a1)\n",
    "        a3 = self.ae3(a2)\n",
    "\n",
    "        if self.training:\n",
    "            return a3\n",
    "\n",
    "        else:\n",
    "            return a3, self.reconstruct(a3)\n",
    "\n",
    "    def reconstruct(self, x):\n",
    "            a2_reconstruct = self.ae3.reconstruct(x)\n",
    "            a1_reconstruct = self.ae2.reconstruct(a2_reconstruct)\n",
    "            x_reconstruct = self.ae1.reconstruct(a1_reconstruct)\n",
    "            return x_reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T23:56:47.666405Z",
     "start_time": "2019-05-03T23:56:47.646207Z"
    }
   },
   "outputs": [],
   "source": [
    "model = StackedAutoEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T23:56:47.672910Z",
     "start_time": "2019-05-03T23:56:47.668389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackedAutoEncoder(\n",
       "  (ae1): CDAutoEncoder(\n",
       "    (forward_pass): Sequential(\n",
       "      (0): Conv2d(3, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (backward_pass): Sequential(\n",
       "      (0): ConvTranspose2d(128, 3, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (criterion): MSELoss()\n",
       "  )\n",
       "  (ae2): CDAutoEncoder(\n",
       "    (forward_pass): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (backward_pass): Sequential(\n",
       "      (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (criterion): MSELoss()\n",
       "  )\n",
       "  (ae3): CDAutoEncoder(\n",
       "    (forward_pass): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (backward_pass): Sequential(\n",
       "      (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (criterion): MSELoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T13:09:35.453517Z",
     "start_time": "2019-05-04T13:09:34.894666Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CDAutoEncoder(nn.Module):\n",
    "    r\"\"\"\n",
    "    Convolutional denoising autoencoder layer for stacked autoencoders.\n",
    "    This module is automatically trained when in model.training is True.\n",
    "    Args:\n",
    "        input_size: The number of features in the input\n",
    "        output_size: The number of features to output\n",
    "        stride: Stride of the convolutional layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, conv_num=1,criterion=nn.MSELoss(),learning_rate=0.01):\n",
    "        super(CDAutoEncoder, self).__init__()\n",
    "        if conv_num==2:\n",
    "            self.forward_pass = nn.Sequential(\n",
    "                nn.Conv2d(input_size, output_size, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                nn.BatchNorm2d(output_size, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(output_size, output_size, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                nn.BatchNorm2d(output_size, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "            )\n",
    "            self.backward_pass = nn.Sequential(\n",
    "                nn.ConvTranspose2d(output_size, output_size, kernel_size=(2, 2), stride=(2, 2)),\n",
    "                nn.BatchNorm2d(output_size, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.ConvTranspose2d(output_size, input_size, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                nn.BatchNorm2d(input_size, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        if conv_num==1:\n",
    "            self.forward_pass = nn.Sequential(\n",
    "                nn.Conv2d(input_size, output_size, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                nn.BatchNorm2d(output_size, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "            )\n",
    "            self.backward_pass = nn.Sequential(\n",
    "                nn.ConvTranspose2d(output_size, input_size, kernel_size=(2, 2), stride=(2, 2)),\n",
    "                nn.BatchNorm2d(input_size, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Train each autoencoder individually\n",
    "        x = x.detach()\n",
    "        # Add noise, but use the original lossless input as the target.\n",
    "        x_noisy = x * (Variable(x.data.new(x.size()).normal_(0, 0.1)) > -.1).type_as(x)\n",
    "        y = self.forward_pass(x_noisy)\n",
    "\n",
    "        if self.training:\n",
    "            x_reconstruct = self.backward_pass(y)\n",
    "            loss = self.criterion(x_reconstruct, Variable(x.data, requires_grad=False))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        return y.detach()\n",
    "\n",
    "    def reconstruct(self, x):\n",
    "        return self.backward_pass(x)\n",
    "\n",
    "\n",
    "class StackedAutoEncoder(nn.Module):\n",
    "    r\"\"\"\n",
    "    A stacked autoencoder made from the convolutional denoising autoencoders above.\n",
    "    Each autoencoder is trained independently and at the same time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, criterion=nn.MSELoss(),learning_rate=0.01):\n",
    "        super(StackedAutoEncoder, self).__init__()\n",
    "\n",
    "        self.ae1 = CDAutoEncoder(3, 64, conv_num=1,criterion=criterion, learning_rate=learning_rate)\n",
    "        self.ae2 = CDAutoEncoder(64, 128, conv_num=1,criterion=criterion, learning_rate=learning_rate)\n",
    "        self.ae3 = CDAutoEncoder(128, 256, conv_num=2,criterion=criterion, learning_rate=learning_rate)\n",
    "        self.ae4 = CDAutoEncoder(256, 512, conv_num=2,criterion=criterion, learning_rate=learning_rate)\n",
    "        self.ae5 = CDAutoEncoder(512, 512, conv_num=2,criterion=criterion, learning_rate=learning_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        a1 = self.ae1(x)\n",
    "        a2 = self.ae2(a1)\n",
    "        a3 = self.ae3(a2)\n",
    "        a4 = self.ae4(a3)\n",
    "        a5 = self.ae5(a4)\n",
    "\n",
    "        if self.training:\n",
    "            return a5, self.reconstruct(a5)\n",
    "\n",
    "        else:\n",
    "            return a5, self.reconstruct(a5)\n",
    "\n",
    "    def reconstruct(self, x):\n",
    "            a4_reconstruct = self.ae5.reconstruct(x)\n",
    "            a3_reconstruct = self.ae4.reconstruct(a4_reconstruct)\n",
    "            a2_reconstruct = self.ae3.reconstruct(a3_reconstruct)\n",
    "            a1_reconstruct = self.ae2.reconstruct(a2_reconstruct)\n",
    "            x_reconstruct = self.ae1.reconstruct(a1_reconstruct)\n",
    "            return x_reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T01:20:37.887384Z",
     "start_time": "2019-05-04T01:20:37.883678Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion_ft = nn.MSELoss()\n",
    "\n",
    "learning_rate_ft=0.01#args.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T01:20:38.543840Z",
     "start_time": "2019-05-04T01:20:38.332418Z"
    }
   },
   "outputs": [],
   "source": [
    "model = StackedAutoEncoder(criterion=criterion_ft,learning_rate=learning_rate_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T01:20:41.091984Z",
     "start_time": "2019-05-04T01:20:41.085010Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackedAutoEncoder(\n",
       "  (ae1): CDAutoEncoder(\n",
       "    (forward_pass): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace)\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (backward_pass): Sequential(\n",
       "      (0): ConvTranspose2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace)\n",
       "    )\n",
       "    (criterion): MSELoss()\n",
       "  )\n",
       "  (ae2): CDAutoEncoder(\n",
       "    (forward_pass): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace)\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (backward_pass): Sequential(\n",
       "      (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace)\n",
       "    )\n",
       "    (criterion): MSELoss()\n",
       "  )\n",
       "  (ae3): CDAutoEncoder(\n",
       "    (forward_pass): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace)\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace)\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (backward_pass): Sequential(\n",
       "      (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace)\n",
       "      (3): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace)\n",
       "    )\n",
       "    (criterion): MSELoss()\n",
       "  )\n",
       "  (ae4): CDAutoEncoder(\n",
       "    (forward_pass): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace)\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace)\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (backward_pass): Sequential(\n",
       "      (0): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace)\n",
       "      (3): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace)\n",
       "    )\n",
       "    (criterion): MSELoss()\n",
       "  )\n",
       "  (ae5): CDAutoEncoder(\n",
       "    (forward_pass): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace)\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace)\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (backward_pass): Sequential(\n",
       "      (0): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace)\n",
       "      (3): ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace)\n",
       "    )\n",
       "    (criterion): MSELoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    since = time.time()\n",
    "    loss_history = []\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, _ in dataloaders['unlabeled']:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(device)\n",
    "            _, inputs_reconstructed = model(inputs)\n",
    "            i_o_loss = criterion(inputs,inputs_reconstructed)\n",
    "            running_loss += i_o_loss.item() * inputs.size(0)\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T23:58:19.406287Z",
     "start_time": "2019-05-03T23:58:19.402055Z"
    }
   },
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Dont want execute the following",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Dont want execute the following\n"
     ]
    }
   ],
   "source": [
    "sys.exit('Dont want execute the following')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T23:56:47.820056Z",
     "start_time": "2019-05-03T23:56:47.624Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "#from model import StackedAutoEncoder\n",
    "\n",
    "if not os.path.exists('./imgs'):\n",
    "    os.mkdir('./imgs')\n",
    "\n",
    "def to_img(x):\n",
    "    x = x.view(x.size(0), 3, 32, 32)\n",
    "    return x\n",
    "\n",
    "num_epochs = 1000\n",
    "batch_size = 128\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    #transforms.RandomRotation(360),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0, hue=0),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = CIFAR10('../data/cifar10/', transform=img_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "model = StackedAutoEncoder().cuda()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch % 10 == 0:\n",
    "        # Test the quality of our features with a randomly initialzed linear classifier.\n",
    "        classifier = nn.Linear(512 * 16, 10).cuda()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "    model.train()\n",
    "    total_time = time.time()\n",
    "    correct = 0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        img, target = data\n",
    "        target = Variable(target).cuda()\n",
    "        img = Variable(img).cuda()\n",
    "        features = model(img).detach()\n",
    "        prediction = classifier(features.view(features.size(0), -1))\n",
    "        loss = criterion(prediction, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = prediction.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    total_time = time.time() - total_time\n",
    "\n",
    "    model.eval()\n",
    "    img, _ = data\n",
    "    img = Variable(img).cuda()\n",
    "    features, x_reconstructed = model(img)\n",
    "    reconstruction_loss = torch.mean((x_reconstructed.data - img.data)**2)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Saving epoch {}\".format(epoch))\n",
    "        orig = to_img(img.cpu().data)\n",
    "        save_image(orig, './imgs/orig_{}.png'.format(epoch))\n",
    "        pic = to_img(x_reconstructed.cpu().data)\n",
    "        save_image(pic, './imgs/reconstruction_{}.png'.format(epoch))\n",
    "\n",
    "    print(\"Epoch {} complete\\tTime: {:.4f}s\\t\\tLoss: {:.4f}\".format(epoch, total_time, reconstruction_loss))\n",
    "    print(\"Feature Statistics\\tMean: {:.4f}\\t\\tMax: {:.4f}\\t\\tSparsity: {:.4f}%\".format(\n",
    "        torch.mean(features.data), torch.max(features.data), torch.sum(features.data == 0.0)*100 / features.data.numel())\n",
    "    )\n",
    "    print(\"Linear classifier performance: {}/{} = {:.2f}%\".format(correct, len(dataloader)*batch_size, 100*float(correct) / (len(dataloader)*batch_size)))\n",
    "    print(\"=\"*80)\n",
    "\n",
    "torch.save(model.state_dict(), './CDAE.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
