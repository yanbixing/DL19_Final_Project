{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T13:30:59.377972Z",
     "start_time": "2019-05-09T13:30:58.880875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 1.0.0\n",
      "Torchvision Version: 0.2.1\n",
      "Warning, CPU mode, pls check\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import sys\n",
    "import copy\n",
    "# import warnings\n",
    "# import random\n",
    "# import shutil\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# import torch.backends.cudnn as cudnn\n",
    "# import torch.distributed as dist\n",
    "# import torch.optim as optim\n",
    "# import torch.multiprocessing as mp\n",
    "\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "###############################################################################################\n",
    "################################### Report Environment ########################################\n",
    "###############################################################################################\n",
    "\n",
    "sys.stdout.write(\"PyTorch Version: {}\\n\".format(torch.__version__))\n",
    "sys.stdout.write(\"Torchvision Version: {}\\n\".format(torchvision.__version__))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    sys.stdout.write('GPU mode \\n')\n",
    "else:\n",
    "    sys.stdout.write('Warning, CPU mode, pls check\\n')\n",
    "\n",
    "############################################################################################\n",
    "######################################     Read Args     ###################################\n",
    "############################################################################################\n",
    "\n",
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "parser = argparse.ArgumentParser(description='DL19_FinalProject_PyTorch')\n",
    "\n",
    "parser.add_argument('--model', type=str, default='vgg',\n",
    "                    help='type of cnn (\"resnet\", \"alexnet\",\"vgg\",\"squeezenet\",\"densenet\",\"inception\")')\n",
    "\n",
    "parser.add_argument('--AE-folder', type=str, default='/beegfs/by783/DL_Final_models/',\n",
    "                    help='path to store model files')\n",
    "\n",
    "parser.add_argument('--AE-file', type=str, default = '190425_raw_vggae_fromscratch_s.pt',\n",
    "                    help='path to autoencoder')\n",
    "\n",
    "parser.add_argument('-b', '--batch-size', default=256, type=int,\n",
    "                    metavar='N',\n",
    "                    help='mini-batch size (default: 256), this is the total '\n",
    "                         'batch size of all GPUs on the current node when '\n",
    "                         'using Data Parallel or Distributed Data Parallel')\n",
    "parser.add_argument('--save-folder', type=str, default='/beegfs/by783/DL_Final_models/',\n",
    "                    help='path to save the final model')\n",
    "\n",
    "parser.add_argument('--save', type=str, default='model.pt',\n",
    "                    help='path to save the final model')\n",
    "parser.add_argument('--num-classes', type=int, default=1000,\n",
    "                    help='number of classes')\n",
    "parser.add_argument(\"--feature-pinning\", type=str, default='True',\n",
    "                    help=\"pin all the conv layers.\")\n",
    "parser.add_argument('--epochs', type=int, default=50,\n",
    "                    help='upper epoch limit')\n",
    "parser.add_argument('--lr', type=float, default=0.001,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                    help='momentum')\n",
    "# parser.add_argument('--noise-level', type=float, default=0.3,\n",
    "#                     help='add noise to input')\n",
    "# no noise added now\n",
    "parser.add_argument('--dataset-path', type=str, default='/beegfs/by783/DL_Final/ssl_data_96',\n",
    "                    help='path to dataset')\n",
    "\n",
    "#args = parser.parse_args()\n",
    "args=parser.parse_args(\"--model vgg --AE-file XXXXXXXX --batch-size 512 --feature-pinning True --save 190505_try2 --epochs 50 --lr 0.001 \".split())\n",
    "########################################################################################\n",
    "\n",
    "model_name = args.model\n",
    "\n",
    "model_load_path = args.AE_folder + args.AE_file\n",
    "\n",
    "save_path = args.save_folder + args.save\n",
    "\n",
    "feature_pinning=str2bool(args.feature_pinning)\n",
    "num_classes = args.num_classes\n",
    "\n",
    "num_epochs = args.epochs\n",
    "loader_batch_size = args.batch_size\n",
    "loader_image_path = args.dataset_path\n",
    "# noise_level = args.noise_level\n",
    "\n",
    "######################################################################################################\n",
    "############################################  Classes  ################################################\n",
    "######################################################################################################\n",
    "\n",
    "\n",
    "class CDAutoEncoder(nn.Module):\n",
    "    r\"\"\"\n",
    "    Convolutional denoising autoencoder layer for stacked autoencoders.\n",
    "    This module is automatically trained when in model.training is True.\n",
    "    Args:\n",
    "        input_size: The number of features in the input\n",
    "        output_size: The number of features to output\n",
    "        stride: Stride of the convolutional layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size, conv_num=1, criterion=nn.MSELoss(), learning_rate=0.01):\n",
    "        super(CDAutoEncoder, self).__init__()\n",
    "        if conv_num == 2:\n",
    "            self.forward_pass = nn.Sequential(\n",
    "                nn.Conv2d(input_size, output_size, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                nn.BatchNorm2d(output_size, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(output_size, output_size, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                nn.BatchNorm2d(output_size, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "            )\n",
    "            self.backward_pass = nn.Sequential(\n",
    "                nn.ConvTranspose2d(output_size, output_size, kernel_size=(2, 2), stride=(2, 2)),\n",
    "                nn.BatchNorm2d(output_size, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.ConvTranspose2d(output_size, input_size, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                nn.BatchNorm2d(input_size, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        if conv_num == 1:\n",
    "            self.forward_pass = nn.Sequential(\n",
    "                nn.Conv2d(input_size, output_size, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                nn.BatchNorm2d(output_size, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "            )\n",
    "            self.backward_pass = nn.Sequential(\n",
    "                nn.ConvTranspose2d(output_size, input_size, kernel_size=(2, 2), stride=(2, 2)),\n",
    "                nn.BatchNorm2d(input_size, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Train each autoencoder individually\n",
    "        x = x.detach()\n",
    "        # Add noise, but use the original lossless input as the target.SGD(self.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        x_noisy = x * (Variable(x.data.new(x.size()).normal_(0, 0.1)) > -.1).type_as(x)\n",
    "        #         print('forward: x: ',x.shape)\n",
    "        y = self.forward_pass(x_noisy)\n",
    "\n",
    "        if self.training:\n",
    "            x_reconstruct = self.backward_pass(y)\n",
    "            #             print('forward: x_re: ',x_reconstruct.shape)\n",
    "            loss = self.criterion(x_reconstruct, Variable(x.data, requires_grad=False))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        return y.detach()\n",
    "\n",
    "    def reconstruct(self, x):\n",
    "        return self.backward_pass(x)\n",
    "\n",
    "\n",
    "class StackedAutoEncoder(nn.Module):\n",
    "    r\"\"\"\n",
    "    A stacked autoencoder made from the convolutional denoising autoencoders above.\n",
    "    Each autoencoder is trained independently and at the same time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, criterion=nn.MSELoss(), learning_rate=0.01):\n",
    "        super(StackedAutoEncoder, self).__init__()\n",
    "\n",
    "        self.ae1 = CDAutoEncoder(3, 64, conv_num=1, criterion=criterion, learning_rate=learning_rate)\n",
    "        self.ae2 = CDAutoEncoder(64, 128, conv_num=1, criterion=criterion, learning_rate=learning_rate)\n",
    "        self.ae3 = CDAutoEncoder(128, 256, conv_num=2, criterion=criterion, learning_rate=learning_rate)\n",
    "        self.ae4 = CDAutoEncoder(256, 512, conv_num=2, criterion=criterion, learning_rate=learning_rate)\n",
    "        self.ae5 = CDAutoEncoder(512, 512, conv_num=2, criterion=criterion, learning_rate=learning_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = self.ae1(x)\n",
    "        a2 = self.ae2(a1)\n",
    "        a3 = self.ae3(a2)\n",
    "        a4 = self.ae4(a3)\n",
    "        a5 = self.ae5(a4)\n",
    "\n",
    "        if self.training:\n",
    "            return a5, self.reconstruct(a5)\n",
    "\n",
    "        else:\n",
    "            return a5, self.reconstruct(a5)\n",
    "\n",
    "    def reconstruct(self, x):\n",
    "        a4_reconstruct = self.ae5.reconstruct(x)\n",
    "        a3_reconstruct = self.ae4.reconstruct(a4_reconstruct)\n",
    "        a2_reconstruct = self.ae3.reconstruct(a3_reconstruct)\n",
    "        a1_reconstruct = self.ae2.reconstruct(a2_reconstruct)\n",
    "        x_reconstruct = self.ae1.reconstruct(a1_reconstruct)\n",
    "        return x_reconstruct\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "class AE_Transfered_Network(torch.nn.Module):\n",
    "    def __init__(self, classifier_type, autoencoder_model, num_classes=1000):\n",
    "        super(AE_Transfered_Network, self).__init__()\n",
    "        if classifier_type != 'vgg':\n",
    "            sys.stdout.write('Dear, we only support vgg now...\\n')\n",
    "\n",
    "        self.features = nn.Sequential(*copy.deepcopy(\n",
    "            list(autoencoder_model.ae1.forward_pass.children()) +\n",
    "            list(autoencoder_model.ae2.forward_pass.children()) +\n",
    "            list(autoencoder_model.ae3.forward_pass.children()) +\n",
    "            list(autoencoder_model.ae4.forward_pass.children()) +\n",
    "            list(autoencoder_model.ae5.forward_pass.children())))\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(3, 3))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=4608, out_features=4096, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=4096, out_features=num_classes, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "########################################################################################\n",
    "##########################################  Functions   ##############################################\n",
    "########################################################################################\n",
    "\n",
    "def image_loader(path, batch_size):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            #transforms.Resize(input_size),\n",
    "            #transforms.CenterCrop(input_size),\n",
    "            # use model fitted with the image size, so no need to resize\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.502, 0.474, 0.426], [0.227, 0.222, 0.226])\n",
    "            # https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "            # [mean],[std] for different channels\n",
    "        ]\n",
    "    )\n",
    "    sup_train_data = datasets.ImageFolder('{}/{}/train'.format(path, 'supervised'), transform=transform)\n",
    "    sup_val_data = datasets.ImageFolder('{}/{}/val'.format(path, 'supervised'), transform=transform)\n",
    "    unsup_data = datasets.ImageFolder('{}/{}/'.format(path, 'unsupervised'), transform=transform)\n",
    "    # source code: https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n",
    "    # Main idea:\n",
    "    data_loader_sup_train = torch.utils.data.DataLoader(\n",
    "        sup_train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    data_loader_sup_val = torch.utils.data.DataLoader(\n",
    "        sup_val_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    data_loader_unsup = torch.utils.data.DataLoader(\n",
    "        unsup_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    print('sup_train_data.class_to_idx==sup_val_data.class_to_idx: ',\n",
    "          sup_train_data.class_to_idx == sup_val_data.class_to_idx)\n",
    "\n",
    "    return data_loader_sup_train, data_loader_sup_val, data_loader_unsup, sup_train_data.class_to_idx\n",
    "\n",
    "######################################################################################################\n",
    "def set_parameter_pin_grad(model, pinning):\n",
    "    if pinning:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    else:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    #best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "            # 切换phase重置loss\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            sys.stdout.write('{} Loss: {:.4f} Acc: {:.4f}\\n'.format(phase, epoch_loss, epoch_acc))\n",
    "            sys.stdout.write('training time: {:.0f}s\\n'.format( time.time() - since ))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val':\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    #best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    with open(save_path, 'wb') as f:\n",
    "                        torch.save(model, f)\n",
    "                #else:\n",
    "                    #lr/=4\n",
    "                # 只有phase为val的acc loss才被加入 val_acc\n",
    "                val_acc_history.append(epoch_acc)\n",
    "                with open(save_path+'_val_acc', 'w') as f:\n",
    "                    for item in val_acc_history:\n",
    "                        f.write(\"%s\\n\" % item)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    sys.stdout.write('Training complete in {:.0f}m {:.0f}s\\n'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    sys.stdout.write('Best val Acc: {:4f}\\n'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T13:33:10.420409Z",
     "start_time": "2019-05-09T13:33:09.958686Z"
    }
   },
   "outputs": [],
   "source": [
    "model_ae=StackedAutoEncoder()\n",
    "\n",
    "model_ft = AE_Transfered_Network('vgg',model_ae)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T13:34:07.757324Z",
     "start_time": "2019-05-09T13:34:07.753625Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace)\n",
       "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU(inplace)\n",
       "  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): ReLU(inplace)\n",
       "  (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (13): ReLU(inplace)\n",
       "  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (17): ReLU(inplace)\n",
       "  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (20): ReLU(inplace)\n",
       "  (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (24): ReLU(inplace)\n",
       "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (27): ReLU(inplace)\n",
       "  (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_parameter_pin_grad(model_ft.features,feature_pinning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
