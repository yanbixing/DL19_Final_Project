{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T18:38:30.027156Z",
     "start_time": "2019-05-04T18:38:29.378333Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "#import numpy as np\n",
    "import argparse\n",
    "#import random\n",
    "#import shutil\n",
    "import time\n",
    "#import warnings\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.parallel\n",
    "#import torch.backends.cudnn as cudnn\n",
    "#import torch.distributed as dist\n",
    "#import torch.optim as optim\n",
    "\n",
    "#import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms#, models\n",
    "#import os\n",
    "import sys\n",
    "\n",
    "from torch.autograd import Variable\n",
    "#import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T18:38:30.060106Z",
     "start_time": "2019-05-04T18:38:30.030224Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--lr'], dest='lr', nargs=None, const=None, default=0.001, type=<class 'float'>, choices=None, help='learning rate', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "parser = argparse.ArgumentParser(description='DL19_FinalProject_PyTorch')\n",
    "\n",
    "parser.add_argument('--model', type=str, default='densenet',\n",
    "                    help='type of cnn (\"resnet\", \"alexnet\",\"vgg\",\"squeezenet\",\"densenet\",\"inception\")')\n",
    "# parser.add_argument('--model-folder', type=str, default='/scratch/by783/DL_Final_models/',\n",
    "#                     help='path to store model files')\n",
    "\n",
    "# parser.add_argument('--model-file', type=str, default = '190425_raw_vggae_fromscratch_s.pt',\n",
    "#                     help='path to autoencoder')\n",
    "\n",
    "parser.add_argument('-b', '--batch-size', default=256, type=int,\n",
    "                    metavar='N',\n",
    "                    help='mini-batch size (default: 256), this is the total '\n",
    "                         'batch size of all GPUs on the current node when '\n",
    "                         'using Data Parallel or Distributed Data Parallel')\n",
    "parser.add_argument('--save-folder', type=str, default='/scratch/by783/DL_Final_models/',\n",
    "                    help='path to save the final model')\n",
    "\n",
    "parser.add_argument('--save', type=str, default='model.pt',\n",
    "                    help='path to save the final model')\n",
    "parser.add_argument('--num-classes', type=int, default=1000,\n",
    "                    help='number of classes')\n",
    "parser.add_argument('--epochs', type=int, default=25,\n",
    "                    help='upper epoch limit')\n",
    "parser.add_argument(\"--feature-pinning\", type=str, default='False',\n",
    "                    help=\"pin all the conv layers.\")\n",
    "parser.add_argument('--noise-level', type=float, default=0.3,\n",
    "                    help='add noise to input')\n",
    "# no noise added now\n",
    "parser.add_argument('--dataset-path', type=str, default='/scratch/by783/DL_Final/ssl_data_96',\n",
    "                    help='path to dataset')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.001,\n",
    "                    help='learning rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T18:38:30.066106Z",
     "start_time": "2019-05-04T18:38:30.062516Z"
    }
   },
   "outputs": [],
   "source": [
    "args=parser.parse_args(\"--model vgg --batch-size 1024 --save 190504_try --lr 0.001\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T18:38:30.073856Z",
     "start_time": "2019-05-04T18:38:30.068683Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = args.model\n",
    "\n",
    "# model_load_path = args.model_folder + args.model_file\n",
    "\n",
    "save_path = args.save_folder + args.save\n",
    "\n",
    "feature_pinning=str2bool(args.feature_pinning)\n",
    "num_classes = args.num_classes\n",
    "\n",
    "num_epochs = args.epochs\n",
    "loader_batch_size = args.batch_size\n",
    "loader_image_path = args.dataset_path\n",
    "noise_level = args.noise_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T18:38:30.172717Z",
     "start_time": "2019-05-04T18:38:30.076043Z"
    }
   },
   "outputs": [],
   "source": [
    "class CDAutoEncoder(nn.Module):\n",
    "    r\"\"\"\n",
    "    Convolutional denoising autoencoder layer for stacked autoencoders.\n",
    "    This module is automatically trained when in model.training is True.\n",
    "    Args:\n",
    "        input_size: The number of features in the input\n",
    "        output_size: The number of features to output\n",
    "        stride: Stride of the convolutional layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, conv_num=1,criterion=nn.MSELoss(),learning_rate=0.01):\n",
    "        super(CDAutoEncoder, self).__init__()\n",
    "        if conv_num==2:\n",
    "            self.forward_pass = nn.Sequential(\n",
    "                nn.Conv2d(input_size, output_size, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                nn.BatchNorm2d(output_size, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(output_size, output_size, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                nn.BatchNorm2d(output_size, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "            )\n",
    "            self.backward_pass = nn.Sequential(\n",
    "                nn.ConvTranspose2d(output_size, output_size, kernel_size=(2, 2), stride=(2, 2)),\n",
    "                nn.BatchNorm2d(output_size, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.ConvTranspose2d(output_size, input_size, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                nn.BatchNorm2d(input_size, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        if conv_num==1:\n",
    "            self.forward_pass = nn.Sequential(\n",
    "                nn.Conv2d(input_size, output_size, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                nn.BatchNorm2d(output_size, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "            )\n",
    "            self.backward_pass = nn.Sequential(\n",
    "                nn.ConvTranspose2d(output_size, input_size, kernel_size=(2, 2), stride=(2, 2)),\n",
    "                nn.BatchNorm2d(input_size, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Train each autoencoder individually\n",
    "        x = x.detach()\n",
    "        # Add noise, but use the original lossless input as the target.\n",
    "        x_noisy = x * (Variable(x.data.new(x.size()).normal_(0, 0.1)) > -.1).type_as(x)\n",
    "#         print('forward: x: ',x.shape)\n",
    "        y = self.forward_pass(x_noisy)\n",
    "\n",
    "        if self.training:\n",
    "            x_reconstruct = self.backward_pass(y)\n",
    "#             print('forward: x_re: ',x_reconstruct.shape)\n",
    "            loss = self.criterion(x_reconstruct, Variable(x.data, requires_grad=False))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        return y.detach()\n",
    "\n",
    "    def reconstruct(self, x):\n",
    "        return self.backward_pass(x)\n",
    "\n",
    "\n",
    "class StackedAutoEncoder(nn.Module):\n",
    "    r\"\"\"\n",
    "    A stacked autoencoder made from the convolutional denoising autoencoders above.\n",
    "    Each autoencoder is trained independently and at the same time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, criterion=nn.MSELoss(),learning_rate=0.01):\n",
    "        super(StackedAutoEncoder, self).__init__()\n",
    "\n",
    "        self.ae1 = CDAutoEncoder(3, 64, conv_num=1,criterion=criterion, learning_rate=learning_rate)\n",
    "        self.ae2 = CDAutoEncoder(64, 128, conv_num=1,criterion=criterion, learning_rate=learning_rate)\n",
    "        self.ae3 = CDAutoEncoder(128, 256, conv_num=2,criterion=criterion, learning_rate=learning_rate)\n",
    "        self.ae4 = CDAutoEncoder(256, 512, conv_num=2,criterion=criterion, learning_rate=learning_rate)\n",
    "        self.ae5 = CDAutoEncoder(512, 512, conv_num=2,criterion=criterion, learning_rate=learning_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        a1 = self.ae1(x)\n",
    "        a2 = self.ae2(a1)\n",
    "        a3 = self.ae3(a2)\n",
    "        a4 = self.ae4(a3)\n",
    "        a5 = self.ae5(a4)\n",
    "\n",
    "        if self.training:\n",
    "            return a5, self.reconstruct(a5)\n",
    "\n",
    "        else:\n",
    "            return a5, self.reconstruct(a5)\n",
    "\n",
    "    def reconstruct(self, x):\n",
    "            a4_reconstruct = self.ae5.reconstruct(x)\n",
    "            a3_reconstruct = self.ae4.reconstruct(a4_reconstruct)\n",
    "            a2_reconstruct = self.ae3.reconstruct(a3_reconstruct)\n",
    "            a1_reconstruct = self.ae2.reconstruct(a2_reconstruct)\n",
    "            x_reconstruct = self.ae1.reconstruct(a1_reconstruct)\n",
    "            return x_reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T18:38:30.197245Z",
     "start_time": "2019-05-04T18:38:30.174984Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, num_epochs=25):\n",
    "    since = time.time()\n",
    "    loss_history = []\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    epochcounter=0\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epochcounter+=1\n",
    "        if epochcounter>=10: break\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        trainbatchcounter=0\n",
    "        for inputs, _ in dataloaders['unlabeled']:\n",
    "            trainbatchcounter+=1\n",
    "            if trainbatchcounter>=10: break\n",
    "            \n",
    "            \n",
    "            \n",
    "            print('For check')\n",
    "            #optimizer.zero_grad()\n",
    "            #model.zero_grad()\n",
    "            inputs = inputs.to(device)\n",
    "            _, inputs_reconstructed = model(inputs)\n",
    "            \n",
    "            i_o_loss = criterion(inputs,inputs_reconstructed)\n",
    "            running_loss += i_o_loss.item() * inputs.size(0)\n",
    "            \n",
    "            #i_o_loss = i_o_loss.detach\n",
    "            #inputs_reconstructed = inputs_reconstructed.detach\n",
    "            del i_o_loss\n",
    "            del inputs_reconstructed\n",
    "            \n",
    "            \n",
    "        epoch_loss = running_loss / len(dataloaders['unlabeled'].dataset)\n",
    "        sys.stdout.write('Training time: {:.0f}s \\n'.format(time.time() - since))\n",
    "        sys.stdout.write('Training loss: {:.4f} \\n'.format(epoch_loss))\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        eval_loss = 0.0\n",
    "        \n",
    "        valbatch = 0\n",
    "        \n",
    "        for inputs, _ in dataloaders['train']:\n",
    "            valbatch+=1\n",
    "            if valbatch>=10: break\n",
    "            print('For check')\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            ouput_feature, inputs_reconstructed = model(inputs)\n",
    "            \n",
    "            \n",
    "            loss = criterion(inputs, inputs_reconstructed)\n",
    "            eval_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            ouput_feature=ouput_feature.detach\n",
    "            loss = loss.detach\n",
    "            inputs_reconstructed = inputs_reconstructed.detach\n",
    "            del ouput_feature\n",
    "            del loss\n",
    "            del inputs_reconstructed\n",
    "            \n",
    "\n",
    "        epoch_eval_loss = eval_loss / len(dataloaders['train'].dataset)\n",
    "        sys.stdout.write('Evaluation time: {:.0f}s \\n'.format(time.time() - since))\n",
    "        sys.stdout.write(' Eval loss: {:.4f} \\n'.format(epoch_eval_loss))\n",
    "        \n",
    "        \n",
    "        loss_history.append((epoch_loss, epoch_eval_loss))\n",
    "\n",
    "        if epoch_eval_loss < best_loss:\n",
    "            best_loss = epoch_eval_loss\n",
    "            # best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            with open(save_path, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "\n",
    "        with open(save_path + '_loss', 'w') as f:\n",
    "            for item in loss_history:\n",
    "                f.write(\"unlabeled: %s, labeled: %s \\n,  \" % (item[0], item[1]))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    return model, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T18:38:30.210852Z",
     "start_time": "2019-05-04T18:38:30.199361Z"
    }
   },
   "outputs": [],
   "source": [
    "def image_loader(path, batch_size):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            #transforms.Resize(input_size),\n",
    "            #transforms.CenterCrop(input_size),\n",
    "            # use model fitted with the image size, so no need to resize\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.502, 0.474, 0.426], [0.227, 0.222, 0.226])\n",
    "            # https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "            # [mean],[std] for different channels\n",
    "        ]\n",
    "    )\n",
    "    sup_train_data = datasets.ImageFolder('{}/{}/train'.format(path, 'supervised'), transform=transform)\n",
    "    sup_val_data = datasets.ImageFolder('{}/{}/val'.format(path, 'supervised'), transform=transform)\n",
    "    unsup_data = datasets.ImageFolder('{}/{}/'.format(path, 'unsupervised'), transform=transform)\n",
    "    # source code: https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n",
    "    # Main idea:\n",
    "    data_loader_sup_train = torch.utils.data.DataLoader(\n",
    "        sup_train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    data_loader_sup_val = torch.utils.data.DataLoader(\n",
    "        sup_val_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    data_loader_unsup = torch.utils.data.DataLoader(\n",
    "        unsup_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    print('sup_train_data.class_to_idx==sup_val_data.class_to_idx: ',\n",
    "          sup_train_data.class_to_idx == sup_val_data.class_to_idx)\n",
    "\n",
    "    return data_loader_sup_train, data_loader_sup_val, data_loader_unsup, sup_train_data.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T18:38:30.292865Z",
     "start_time": "2019-05-04T18:38:30.214190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 0.4.1\n",
      "Torchvision Version: 0.2.2\n",
      " GPU mode \n"
     ]
    }
   ],
   "source": [
    "sys.stdout.write(\"PyTorch Version: {}\\n\".format(torch.__version__))\n",
    "sys.stdout.write(\"Torchvision Version: {}\\n \".format(torchvision.__version__))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    sys.stdout.write('GPU mode \\n')\n",
    "else:\n",
    "    sys.stdout.write('Warning, CPU mode, pls check\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T18:38:37.671695Z",
     "start_time": "2019-05-04T18:38:30.295330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Begin to load data...\n",
      "sup_train_data.class_to_idx==sup_val_data.class_to_idx:  True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "####### load data, input_size is used ####\n",
    "\n",
    "sys.stdout.write('\\nBegin to load data...\\n')\n",
    "\n",
    "dataloaders={}\n",
    "\n",
    "dataloaders['train'], dataloaders['val'], dataloaders['unlabeled'], class_to_idx_dict = image_loader(loader_image_path,loader_batch_size)\n",
    "\n",
    "# dataloaders['unlabeled'], dataloaders['train'], _, _ = image_loader(loader_image_path,loader_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T18:38:40.034278Z",
     "start_time": "2019-05-04T18:38:37.674412Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion_ft = nn.MSELoss()\n",
    "\n",
    "learning_rate_ft=args.lr\n",
    "\n",
    "model_ft = StackedAutoEncoder(criterion=criterion_ft,learning_rate=learning_rate_ft)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model_ft = nn.DataParallel(model_ft)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T18:38:40.040644Z",
     "start_time": "2019-05-04T18:38:40.036651Z"
    }
   },
   "outputs": [],
   "source": [
    "#model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:00:32.954522Z",
     "start_time": "2019-05-04T18:38:40.042973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin to train...Epoch 0/24\n",
      "----------\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "Training time: 303s \n",
      "Training loss: 896377.0080 \n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "Evaluation time: 562s \n",
      " Eval loss: 6583389.3760 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/by783/myenv/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type StackedAutoEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/by783/myenv/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type CDAutoEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/24\n",
      "----------\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "Training time: 849s \n",
      "Training loss: 887708.8400 \n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "Evaluation time: 1124s \n",
      " Eval loss: 6626314.3040 \n",
      "Epoch 2/24\n",
      "----------\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "Training time: 1402s \n",
      "Training loss: 908621.9840 \n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "Evaluation time: 1681s \n",
      " Eval loss: 6680360.9600 \n",
      "Epoch 3/24\n",
      "----------\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "Training time: 1952s \n",
      "Training loss: 917894.0560 \n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "Evaluation time: 2210s \n",
      " Eval loss: 6731090.8800 \n",
      "Epoch 4/24\n",
      "----------\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "Training time: 2499s \n",
      "Training loss: 926752.1840 \n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "Evaluation time: 2748s \n",
      " Eval loss: 6737774.0800 \n",
      "Epoch 5/24\n",
      "----------\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "Training time: 3057s \n",
      "Training loss: 928335.1280 \n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "Evaluation time: 3325s \n",
      " Eval loss: 6633868.9920 \n",
      "Epoch 6/24\n",
      "----------\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "Training time: 3635s \n",
      "Training loss: 938330.1760 \n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "Evaluation time: 3862s \n",
      " Eval loss: 6630810.2400 \n",
      "Epoch 7/24\n",
      "----------\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "Training time: 4130s \n",
      "Training loss: 932819.2720 \n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "Evaluation time: 4390s \n",
      " Eval loss: 6619273.4080 \n",
      "Epoch 8/24\n",
      "----------\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "Training time: 4662s \n",
      "Training loss: 942260.3600 \n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "For check\n",
      "Evaluation time: 4913s \n",
      " Eval loss: 6575223.1040 \n",
      "Training complete in 81m 53s\n",
      "Finished"
     ]
    }
   ],
   "source": [
    "sys.stdout.write('Begin to train...')\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders, criterion_ft, num_epochs=num_epochs)\n",
    "\n",
    "sys.stdout.write('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1024\n",
    "no detach, no del: 4 epoch \n",
    "no del, detach: 3 epoch\n",
    "no detach, del: ok funished\n",
    "detach, del: >5 batch no problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:18:28.368286Z",
     "start_time": "2019-05-04T20:18:28.352930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1573s for train set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
